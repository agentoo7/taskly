# Story 3.6: Commit History Timeline

## Status
Draft

---

## Story

**As a** team member,
**I want** to see commit history for linked PRs in the card timeline,
**so that** I can track code progress without leaving the board.

---

## Acceptance Criteria

1. Card activity timeline shows commits from linked PRs
2. Commit entries display: commit message (first line), author avatar, timestamp, SHA (short)
3. Clicking commit opens GitHub commit page in new tab
4. Webhook `push` event to PR branch triggers commit sync
5. Commits stored in `pull_requests.commit_count` and fetched on-demand from GitHub API
6. Timeline filters allow showing/hiding commits (toggle: "Show commits")
7. Commit messages support markdown links and issue references (#123)
8. Batch commit display: if >5 commits pushed at once, show summary "6 commits pushed" with expand button
9. Commits grouped by PR: timeline shows "PR #123" header with commits nested below
10. Force-pushed commits marked with warning icon and tooltip: "Force pushed (history rewritten)"

---

## Tasks / Subtasks

- [ ] **Task 1: Create commit data model** (AC: 5)
  - [ ] Create `Commit` model or store in PR metadata
  - [ ] Fields: sha, message, author_name, author_avatar, timestamp, pr_id

- [ ] **Task 2: Handle push webhook events** (AC: 4)
  - [ ] Parse `push` event for PR branches
  - [ ] Extract commit data from payload
  - [ ] Store commits or update PR commit_count

- [ ] **Task 3: Add commits to timeline** (AC: 1, 2, 3)
  - [ ] Fetch commits when loading card timeline
  - [ ] Display commit entries with author and message
  - [ ] Link to GitHub commit page

- [ ] **Task 4: Add timeline filters** (AC: 6)
  - [ ] Toggle to show/hide commits
  - [ ] Filter stored in user preferences

- [ ] **Task 5: Write tests** (AC: 1-10)
  - [ ] Integration test: Push webhook creates commit entries
  - [ ] Component test: Timeline displays commits

---

## Dev Notes

### Commit Storage Strategy

**Problem:** Storing all commits for every PR creates database bloat and performance issues.

**Solution: Hybrid Storage Strategy**

1. **Recent Commits Stored (Last 50 per PR)**
   - Most recent 50 commits per PR stored in database
   - Covers 95% of PR sizes (median PR has ~10 commits)
   - Enables fast timeline rendering without GitHub API calls

2. **Full History via GitHub API**
   - If PR has >50 commits, show "View all X commits on GitHub" link
   - On-demand fetch via GitHub API for historical commits
   - Cache fetched commits in Redis (TTL: 1 hour)

3. **Commit Summary for Large PRs**
   - PRs with >50 commits show summary: "View 127 commits on GitHub"
   - Timeline shows: First 10 commits, expand button, last 10 commits
   - Prevents UI overload with hundreds of commits

**Database Schema:**

```python
class CommitCache(Base):
    """Cache of recent commits for PR timeline."""
    __tablename__ = "commit_cache"

    id = Column(UUID, primary_key=True, default=uuid.uuid4)
    pr_id = Column(UUID, ForeignKey("pull_requests.id", ondelete="CASCADE"), nullable=False, index=True)
    sha = Column(String(40), unique=True, nullable=False, index=True)
    message = Column(Text, nullable=False)
    author_login = Column(String(100))
    author_avatar_url = Column(String(500))
    author_date = Column(DateTime(timezone=True), nullable=False)
    html_url = Column(String(500))
    additions = Column(Integer, default=0)
    deletions = Column(Integer, default=0)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Composite index for efficient PR commit queries
    __table_args__ = (
        Index("ix_commit_cache_pr_date", "pr_id", "author_date"),
    )
```

**Storage Limits:**

| PR Size | Storage Approach | Rationale |
|---------|------------------|-----------|
| ‚â§50 commits | Store all in database | Fast rendering, no API calls |
| 51-200 commits | Store recent 50, link to GitHub for rest | Balance between performance and completeness |
| >200 commits | Store recent 50, show summary badge | Prevent UI/DB overload |

### Large PR Handling

**Scenario: PR with 500+ commits (e.g., merge from long-running branch)**

**UI Strategy:**

1. **Collapsed View (Default):**
   ```
   üì¶ 127 commits
   [Expand to see commits] [View all on GitHub ‚Üí]
   ```

2. **Expanded View (After clicking "Expand"):**
   ```
   First 10 commits:
   - abc1234 Fix bug in auth service
   - def5678 Add user profile endpoint
   ...

   [Load 107 more commits] [View all on GitHub ‚Üí]

    Last 10 commits:
   - xyz9876 Merge branch 'main' into feature
   - uvw5432 Update dependencies
   ```

3. **Pagination on Expand:**
   - Load commits in batches of 20 via GitHub API
   - Show loading spinner while fetching
   - Cache fetched commits in Redis (1 hour TTL)

**Performance Considerations:**

- Timeline initial render: <100ms (only fetches 50 stored commits)
- "Load more" action: 300-500ms (GitHub API call + cache)
- Maximum commits shown in UI: 200 (prevents browser slowdown)

### Force-Push Handling

**Scenario: Developer force-pushes PR branch, rewriting history**

**Problem:** Old commits in database no longer exist on GitHub, causing 404 errors when clicked.

**Solution: Orphaned Commit Detection & Marking**

1. **On Force-Push Webhook Event:**
   - GitHub sends `push` event with `"forced": true` flag
   - Mark all existing commits for that PR as `orphaned = true`
   - Fetch new commit history from GitHub API
   - Store new commits with `orphaned = false`

2. **Display Orphaned Commits:**
   - Show with ‚ö†Ô∏è warning icon and strikethrough text
   - Tooltip: "Force pushed (history rewritten)"
   - Clicking orphaned commit shows message: "This commit no longer exists (force pushed)"
   - Keep orphaned commits for audit trail (don't delete)

3. **Timeline Visualization:**
   ```
   ‚ö†Ô∏è History rewritten via force push
   ‚îú‚îÄ Old commits (before force push):
   ‚îÇ  ‚îú‚îÄ ‚ö†Ô∏è abc1234 Old implementation (orphaned)
   ‚îÇ  ‚îî‚îÄ ‚ö†Ô∏è def5678 Fix typo (orphaned)
   ‚îî‚îÄ New commits (after force push):
      ‚îú‚îÄ xyz9876 Refactored implementation
      ‚îî‚îÄ uvw5432 Updated tests
   ```

**Database Update:**

```python
# Add orphaned flag to CommitCache model
orphaned = Column(Boolean, default=False, nullable=False)
force_push_detected_at = Column(DateTime(timezone=True), nullable=True)
```

**Webhook Handler:**

```python
async def handle_push_event(payload: dict):
    """Handle push webhook, detect force pushes."""
    if payload.get("forced"):
        pr = await get_pr_for_branch(payload["ref"])
        if pr:
            # Mark old commits as orphaned
            await db.execute(
                update(CommitCache)
                .where(CommitCache.pr_id == pr.id, Commit Cache.orphaned == False)
                .values(orphaned=True, force_push_detected_at=datetime.utcnow())
            )

            # Fetch new commit history
            new_commits = await fetch_pr_commits_from_github(pr.id)
            for commit in new_commits[-50:]:  # Store recent 50
                await upsert_commit(pr.id, commit, orphaned=False)
```

### GitHub API Rate Limiting

**Problem:** Fetching commit history for many PRs can exhaust GitHub API rate limit (5000 requests/hour).

**Mitigation Strategies:**

1. **Batch Fetching:**
   - Group PRs by repository
   - Use GraphQL API to fetch commits for multiple PRs in single request
   - Reduces API calls by 10x

2. **Smart Caching:**
   - Redis cache with 24-hour TTL for commit data
   - Cache key: `commits:{pr_id}:{page}`
   - Invalidate cache on new push event

3. **Priority Queue:**
   - Active PRs (opened in last 7 days): High priority, fetch every 15 min
   - Merged PRs: Low priority, fetch once after merge
   - Closed PRs: No fetching (use cached data)

4. **Rate Limit Monitoring:**
   - Track remaining API calls in Redis
   - If <500 remaining, pause background fetching
   - Alert ops team if rate limit exhausted

**API Call Budget:**

- Per PR: 1 call to fetch commits (or 0 if cached)
- 50 active PRs: 50 calls/15min = 200 calls/hour = 4% of rate limit
- Safe buffer: <1000 calls/hour for commit fetching

### Commit Timeline Entry

```typescript
// frontend/src/components/board/timeline-commit.tsx
export function TimelineCommit({ commit }: { commit: any }) {
  const isOrphaned = commit.orphaned

  return (
    <div className={`flex gap-3 text-sm ${isOrphaned ? 'opacity-60' : ''}`}>
      <Avatar className="h-6 w-6">
        <AvatarImage src={commit.author.avatar_url} />
      </Avatar>
      <div>
        {isOrphaned && (
          <span className="mr-1" title="Force pushed (history rewritten)">‚ö†Ô∏è</span>
        )}
        <a
          href={commit.html_url}
          target="_blank"
          className={`font-mono text-xs ${isOrphaned ? 'line-through' : ''}`}
        >
          {commit.sha.slice(0, 7)}
        </a>
        <p className={isOrphaned ? 'line-through' : ''}>
          {commit.commit.message.split('\n')[0]}
        </p>
        <span className="text-xs text-muted-foreground">
          {commit.author.login} ¬∑ {formatDistanceToNow(new Date(commit.commit.author.date))}
        </span>
      </div>
    </div>
  )
}
```

**Testing Requirements:**
- Integration test: Store 50 commits, verify only recent 50 kept
- Integration test: Force push event marks old commits as orphaned
- Load test: Timeline with 200 commits renders in <500ms
- Integration test: GitHub API rate limit handling (mock 403 response)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | 1.0 | Initial story creation from PRD Epic 3 Story 3.6 | Sarah (PO Agent) |
| 2025-10-23 | 1.1 | Added commit storage strategy (50 commit limit), large PR handling, force-push detection, GitHub API rate limiting | Bob (SM Agent) |
