# Story 2.1: P2 Tasks for 85%+ Coverage Target

**Story:** 2.1 Workspace Creation & Management
**Current Coverage (After P1):** 78-80% (overall project)
**Target Coverage:** 85%+ (stretch goal)
**Gap:** 5-7%
**Total Estimated Effort:** 7 hours

---

## Overview

P1 tasks will bring overall coverage to 78-80%, meeting the 80% target. These P2 tasks are **optional enhancements** to achieve 85%+ coverage for production-grade quality assurance.

**When to implement P2 tasks:**
- After P1 tasks are complete and validated
- If project requires >80% coverage for compliance/policy reasons
- If time permits before release
- As part of technical debt reduction sprints

---

## Coverage Targets After P1

### Expected State After P1 Completion

| Module/Area | Coverage After P1 | Remaining Gaps |
|-------------|-------------------|----------------|
| Workspace | 84-90% | Exception edge cases |
| Auth | 70-75% | GitHub API error scenarios |
| Health | 80%+ | Minimal gaps |
| **Models** | **~60%** | Validations, relationships |
| **Middleware** | **~85%** | Edge cases |
| Core/Config | 80%+ | Minimal gaps |
| Schemas | 90%+ | Minimal gaps |

**Focus Areas for P2:**
1. Model validations and constraints
2. Middleware error scenarios
3. Integration error handling
4. Repository layer (currently 0%)

---

## P2 Task 1: Model Validation Tests

**Priority:** P2 (Nice to Have - Data Integrity)
**Effort:** 3 hours
**Owner:** Backend Developer
**Issue ID:** TEST-007

### Objective

Achieve 80%+ coverage for model validation logic by testing field constraints, relationships, and business rules.

**Impact:** +15-20% coverage for models (60% → 80%+)

### Current Coverage

```
Models Coverage (estimated ~60%):
- workspace.py: 94% (mostly covered)
- workspace_member.py: 94% (mostly covered)
- user.py: 95% (mostly covered)
- board.py: 95% (mostly covered)
- card.py: 97% (mostly covered)

Uncovered areas:
- __repr__ methods (not critical)
- Relationship lazy loading
- Constraint violations (unique, foreign key, check constraints)
- Enum validation
```

### Acceptance Criteria

#### Workspace Model Tests

- [ ] Test: `test_workspace_unique_name_per_user`
  - Given: User creating workspace with duplicate name (if constraint exists)
  - When: Creating second workspace with same name
  - Then: Should succeed (no unique constraint) OR raise IntegrityError

- [ ] Test: `test_workspace_cascade_delete_to_boards`
  - Given: Workspace with 3 boards
  - When: Workspace deleted
  - Then: All 3 boards cascade deleted

- [ ] Test: `test_workspace_created_at_auto_set`
  - Given: Creating new workspace
  - When: Workspace created without explicit created_at
  - Then: created_at automatically set to current time

#### WorkspaceMember Model Tests

- [ ] Test: `test_workspace_member_unique_constraint`
  - Given: User already member of workspace
  - When: Adding same user to workspace again
  - Then: Raises IntegrityError (unique constraint violation)

- [ ] Test: `test_workspace_member_role_enum_validation`
  - Given: Invalid role value
  - When: Creating WorkspaceMember with invalid role
  - Then: Raises ValueError or ValidationError

- [ ] Test: `test_workspace_member_cascade_delete_with_workspace`
  - Given: Workspace with 5 members
  - When: Workspace deleted
  - Then: All 5 memberships cascade deleted

#### User Model Tests

- [ ] Test: `test_user_github_id_unique`
  - Given: Existing user with github_id=12345
  - When: Creating another user with github_id=12345
  - Then: Raises IntegrityError

- [ ] Test: `test_user_email_validation`
  - Given: Invalid email format
  - When: Creating user
  - Then: Accepted (validation happens at Pydantic schema layer)

#### Board Model Tests

- [ ] Test: `test_board_columns_default_empty_list`
  - Given: Creating board without columns
  - When: Board created
  - Then: columns defaults to []

- [ ] Test: `test_board_archived_default_false`
  - Given: Creating board without archived flag
  - When: Board created
  - Then: archived defaults to False

### Implementation Details

**File to Create:** `backend/tests/unit/models/test_models.py`

**Required Fixtures:**
```python
@pytest.fixture
async def workspace_with_boards(db_session: AsyncSession) -> Workspace:
    """Workspace with multiple boards for cascade testing."""
    from uuid import uuid4

    workspace = Workspace(
        name="Test Workspace",
        created_by=uuid4(),
    )
    db_session.add(workspace)
    await db_session.flush()

    # Add 3 boards
    for i in range(3):
        board = Board(
            name=f"Board {i+1}",
            workspace_id=workspace.id,
            columns=[],
        )
        db_session.add(board)

    await db_session.commit()
    await db_session.refresh(workspace)
    return workspace
```

**Example Test:**
```python
@pytest.mark.asyncio
async def test_workspace_member_unique_constraint(
    db_session: AsyncSession,
    test_user: User,
    test_workspace: Workspace,
) -> None:
    """Test that user can only be added once to workspace."""
    from sqlalchemy.exc import IntegrityError

    # Given: User is already member of workspace
    membership1 = WorkspaceMember(
        user_id=test_user.id,
        workspace_id=test_workspace.id,
        role=RoleEnum.MEMBER,
    )
    db_session.add(membership1)
    await db_session.commit()

    # When: Try to add same user to same workspace again
    membership2 = WorkspaceMember(
        user_id=test_user.id,
        workspace_id=test_workspace.id,
        role=RoleEnum.ADMIN,  # Different role, same user+workspace
    )
    db_session.add(membership2)

    # Then: Raises IntegrityError on unique constraint
    with pytest.raises(IntegrityError) as exc_info:
        await db_session.commit()

    assert "unique constraint" in str(exc_info.value).lower()
```

### Validation

Run tests:
```bash
cd backend
uv run pytest tests/unit/models/test_models.py -v --cov=app/models
```

Expected: 10 tests passing, models coverage 80%+

**Estimated Coverage Gain:** +15-20% for models

---

## P2 Task 2: Middleware Edge Cases

**Priority:** P2 (Nice to Have - Production Reliability)
**Effort:** 2 hours
**Owner:** Backend Developer
**Issue ID:** TEST-008

### Objective

Achieve 95%+ coverage for middleware by testing edge cases and error scenarios.

**Impact:** +10% coverage for middleware (85% → 95%+)

### Current Coverage

```
app/api/middleware.py: 83% coverage (24 statements, 4 missed)

Uncovered lines:
- Lines 70-79: Exception handling in correlation ID middleware
```

### Acceptance Criteria

#### Correlation ID Middleware Tests

- [ ] Test: `test_correlation_id_middleware_uses_provided_id`
  - Given: Request with X-Correlation-ID header
  - When: Request processed
  - Then: Uses provided correlation ID (not generated)

- [ ] Test: `test_correlation_id_middleware_generates_id_when_missing`
  - Given: Request WITHOUT X-Correlation-ID header
  - When: Request processed
  - Then: Generates new UUID correlation ID

- [ ] Test: `test_correlation_id_middleware_adds_to_response`
  - Given: Any request
  - When: Request processed
  - Then: Response includes X-Correlation-ID header

- [ ] Test: `test_correlation_id_middleware_handles_request_exception`
  - Given: Request that raises exception during processing
  - When: Exception occurs
  - Then: Logs error with correlation ID and re-raises

- [ ] Test: `test_correlation_id_middleware_clears_context_between_requests`
  - Given: Two sequential requests with different correlation IDs
  - When: Requests processed
  - Then: Each request has its own isolated correlation ID context

### Implementation Details

**File to Create:** `backend/tests/integration/test_middleware.py`

**Required Fixtures:**
```python
@pytest.fixture
def correlation_id() -> str:
    """Generate test correlation ID."""
    return "test-correlation-12345"

@pytest.fixture
def mock_failing_endpoint(app):
    """Add endpoint that raises exception for middleware testing."""
    @app.get("/test/failing")
    async def failing_endpoint():
        raise ValueError("Test exception")

    yield
    # Cleanup: remove route after test
```

**Example Test:**
```python
@pytest.mark.asyncio
async def test_correlation_id_middleware_handles_request_exception(
    client: AsyncClient,
    correlation_id: str,
) -> None:
    """Test that middleware logs errors with correlation ID."""
    import structlog
    from io import StringIO
    import sys

    # Capture log output
    log_capture = StringIO()
    structlog.configure(
        processors=[
            structlog.processors.JSONRenderer(),
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        logger_factory=structlog.PrintLoggerFactory(file=log_capture),
    )

    # When: Request fails with exception
    response = await client.get(
        "/test/failing",  # Endpoint that raises exception
        headers={"X-Correlation-ID": correlation_id},
    )

    # Then: Returns 500 (or appropriate error)
    assert response.status_code == 500

    # And: Logs error with correlation ID
    log_output = log_capture.getvalue()
    assert correlation_id in log_output
    assert "error" in log_output.lower()
```

### Validation

Run tests:
```bash
cd backend
uv run pytest tests/integration/test_middleware.py -v --cov=app/api/middleware
```

Expected: 5 tests passing, middleware.py coverage 95%+

**Estimated Coverage Gain:** +10% for middleware

---

## P2 Task 3: Integration Error Scenarios

**Priority:** P2 (Nice to Have - Production Reliability)
**Effort:** 2 hours
**Owner:** Backend Developer
**Issue ID:** TEST-009

### Objective

Test database transaction rollback, concurrent modifications, and connection error scenarios.

**Impact:** +5% coverage for services/API (error paths)

### Current Coverage Gap

```
Service layer uncovered error scenarios:
- Transaction rollback on nested transaction failure
- Concurrent modification conflicts (optimistic locking)
- Database connection pool exhaustion
- Long-running transaction timeouts
```

### Acceptance Criteria

#### Transaction Rollback Tests

- [ ] Test: `test_workspace_creation_rolls_back_on_membership_failure`
  - Given: Workspace creation succeeds but membership creation fails
  - When: create_workspace() called
  - Then: Entire transaction rolled back (no workspace or membership created)

- [ ] Test: `test_update_workspace_rolls_back_on_constraint_violation`
  - Given: Workspace update violates database constraint
  - When: update_workspace() called
  - Then: Transaction rolled back, workspace remains unchanged

#### Concurrent Modification Tests

- [ ] Test: `test_concurrent_workspace_update_last_write_wins`
  - Given: Two users updating same workspace simultaneously
  - When: Both updates submitted
  - Then: One succeeds, one may fail (or last write wins depending on implementation)

- [ ] Test: `test_concurrent_workspace_delete_and_update`
  - Given: User A deleting workspace while User B updating it
  - When: Both operations submitted
  - Then: Delete succeeds, update fails with 404

#### Database Error Tests

- [ ] Test: `test_workspace_list_handles_database_timeout`
  - Given: Database query times out
  - When: get_user_workspaces() called
  - Then: Raises timeout exception with clear error message

- [ ] Test: `test_workspace_create_handles_connection_pool_exhaustion`
  - Given: Database connection pool exhausted
  - When: create_workspace() called
  - Then: Raises connection error (with retry logic if implemented)

### Implementation Details

**File to Create:** `backend/tests/integration/test_error_scenarios.py`

**Required Fixtures:**
```python
@pytest.fixture
def mock_db_timeout(monkeypatch):
    """Mock database timeout scenario."""
    import asyncio

    async def timeout_query(*args, **kwargs):
        await asyncio.sleep(10)  # Exceed timeout
        raise asyncio.TimeoutError("Query timeout")

    return timeout_query

@pytest.fixture
def mock_connection_pool_exhausted(monkeypatch):
    """Mock connection pool exhaustion."""
    from sqlalchemy.exc import TimeoutError

    async def no_connection(*args, **kwargs):
        raise TimeoutError("QueuePool limit of size X overflow Y reached")

    return no_connection
```

**Example Test:**
```python
@pytest.mark.asyncio
async def test_workspace_creation_rolls_back_on_membership_failure(
    db_session: AsyncSession,
    test_user: User,
    monkeypatch,
) -> None:
    """Test transaction rollback when membership creation fails."""
    from app.services.workspace_service import WorkspaceService

    # Setup: Mock membership creation to fail
    original_add = db_session.add

    def failing_add(instance):
        if isinstance(instance, WorkspaceMember):
            raise Exception("Membership creation failed")
        return original_add(instance)

    monkeypatch.setattr(db_session, "add", failing_add)

    service = WorkspaceService(db_session)

    # When: Try to create workspace
    with pytest.raises(Exception) as exc_info:
        await service.create_workspace(
            name="Test Workspace",
            creator_id=test_user.id,
        )

    assert "Membership creation failed" in str(exc_info.value)

    # Then: Verify no workspace was created (transaction rolled back)
    result = await db_session.execute(
        select(Workspace).where(Workspace.name == "Test Workspace")
    )
    assert result.scalar_one_or_none() is None
```

### Validation

Run tests:
```bash
cd backend
uv run pytest tests/integration/test_error_scenarios.py -v
```

Expected: 6 tests passing, improved error path coverage

**Estimated Coverage Gain:** +5% for services/API

---

## P2 Task 4: Repository Layer Tests (Optional)

**Priority:** P2 (Optional - Currently Unused)
**Effort:** 2 hours (if repository pattern is activated)
**Owner:** Backend Developer
**Issue ID:** TEST-010

### Objective

Achieve 60%+ coverage for repository layer (currently 0% as it's unused).

**Note:** Repository layer is currently **not in use** (services interact directly with models). Only implement if/when repository pattern is activated.

### Current Coverage

```
Repository layer: 0% coverage
- base.py: 0% (28 statements)
- workspace_repository.py: 0% (11 statements)
- user_repository.py: 0% (14 statements)
- board_repository.py: 0% (14 statements)
- card_repository.py: 0% (17 statements)

Status: Currently unused, services query models directly
```

### Acceptance Criteria

**Only implement if repository pattern is activated in future:**

- [ ] Test: `test_workspace_repository_get_by_id`
- [ ] Test: `test_workspace_repository_list_by_user`
- [ ] Test: `test_workspace_repository_create`
- [ ] Test: `test_workspace_repository_update`
- [ ] Test: `test_workspace_repository_delete`

### Implementation Details

**File to Create:** `backend/tests/unit/repositories/test_workspace_repository.py`

**Note:** Skip this task unless repository pattern is activated. Current 0% coverage is acceptable for unused code.

---

## Task Summary & Sequencing

### Recommended Order

1. **P2-1: Model Validation Tests** (3h) - Data integrity verification
2. **P2-2: Middleware Edge Cases** (2h) - Production reliability
3. **P2-3: Integration Error Scenarios** (2h) - Error handling completeness
4. **P2-4: Repository Layer Tests** (SKIP) - Only if pattern activated

**Total:** 7 hours (excluding P2-4)

### Expected Coverage After P2 Tasks

| Module | After P1 | After P2 | Gain |
|--------|----------|----------|------|
| Models | 60% | 80%+ | +20% |
| Middleware | 85% | 95%+ | +10% |
| Services (error paths) | 75% | 80%+ | +5% |
| **Overall Project** | **78-80%** | **85-87%** | **+5-7%** |

---

## Validation Commands

### Run All P2 Tests
```bash
cd backend

# Run all P2 tests
uv run pytest \
  tests/unit/models/test_models.py \
  tests/integration/test_middleware.py \
  tests/integration/test_error_scenarios.py \
  -v

# Check overall coverage
uv run pytest tests/ --cov=app --cov-report=term-missing
```

### Expected Results
- Total tests: ~76 (55 from P0+P1 + ~21 from P2)
- Pass rate: 100%
- Overall coverage: 85-87%

---

## When to Implement P2 Tasks

### Implement P2 tasks if:

✅ **Compliance requirement:** Company policy requires >80% coverage
✅ **Pre-production hardening:** Extra testing before major release
✅ **Technical debt sprint:** Dedicated time for quality improvements
✅ **High-risk production environment:** Medical, financial, or safety-critical systems
✅ **Customer requirement:** Client contract specifies >80% coverage

### Skip P2 tasks if:

⏩ **Time-constrained:** Need to move to next story urgently
⏩ **P1 complete:** 78-80% coverage meets internal DoD
⏩ **MVP phase:** Iterating rapidly, will add later
⏩ **Resource-limited:** Team has higher priority work
⏩ **Not critical path:** Other features blocking release

---

## Cost-Benefit Analysis

### Benefits of P2 Tasks

**Testing Benefits:**
- ✅ Higher confidence in error handling
- ✅ Better coverage of edge cases
- ✅ Validates data integrity constraints
- ✅ Tests production failure scenarios

**Production Benefits:**
- ✅ Fewer bugs in error paths
- ✅ Better observability (middleware logging tested)
- ✅ Graceful degradation verified
- ✅ Concurrent operation safety

**Maintenance Benefits:**
- ✅ Regression detection for models
- ✅ Safer refactoring
- ✅ Documentation via tests

### Costs of P2 Tasks

**Direct Costs:**
- ⏱️ 7 hours development time
- 🧪 21 additional tests to maintain
- 📝 Documentation overhead

**Opportunity Costs:**
- ⚠️ Delays next story by 1 day
- ⚠️ Could implement 1-2 small features instead
- ⚠️ Team bandwidth on tests vs features

### Recommendation

**For Story 2.1:** Complete P1 tasks (reaches 78-80%), defer P2 to technical debt backlog

**Rationale:**
1. P1 achieves 80% target (meets DoD)
2. All critical paths tested (workspace, auth core)
3. P2 tests edge cases, not critical path
4. Can implement P2 incrementally in future sprints
5. Better to deliver features with 80% coverage than delay for 85%

**Create backlog items for P2 tasks, prioritize based on production incidents.**

---

## Gate Decision Criteria

### P1 Completion (78-80% Coverage)

✅ **SUFFICIENT for Story 2.1 PASS:**
- All workspace functionality tested
- All auth core flows tested
- All authorization boundaries tested
- Critical error paths covered
- Meets 80% DoD requirement

### P2 Completion (85-87% Coverage)

✅ **EXCELLENT for production-grade quality:**
- Edge cases thoroughly tested
- Error scenarios verified
- Data integrity constraints validated
- Middleware reliability confirmed
- Exceeds typical industry standards (80%)

---

## Integration with CI/CD

### Coverage Thresholds

**Recommended `pyproject.toml` settings:**

```toml
[tool.pytest.ini_options]
# Fail build if coverage drops below threshold
addopts = """
  -v
  --cov=app
  --cov-report=term-missing
  --cov-report=html
  --cov-fail-under=78
"""

# Per-module thresholds (after P1)
[tool.coverage.report]
fail_under = 78

# Exclude repository layer from coverage (unused)
[tool.coverage.run]
omit = [
    "app/repositories/*",
    "app/tasks/*",
    "app/websockets/*",
]
```

**After P2 (if implemented):**
```toml
fail_under = 85
```

---

## Monitoring Coverage Over Time

### Track Coverage Metrics

**Recommended tracking:**
1. **Per-PR coverage delta** (should not decrease)
2. **Module-specific coverage** (workspace: 80%+, auth: 70%+)
3. **Trend over time** (should increase or stabilize)
4. **Uncovered critical paths** (red flag if critical code uncovered)

**Tools:**
- Codecov (coverage.io)
- SonarQube
- GitHub Actions coverage reports

---

## Notes

1. **P2 is optional enhancement** - P1 meets DoD requirements
2. **Prioritize based on risk** - High-risk modules get P2 first
3. **Incremental implementation** - Can do 1 P2 task per sprint
4. **Monitor production** - Implement P2 tests for areas with bugs
5. **Don't over-test** - 100% coverage has diminishing returns

---

**Document Status:** Ready for Implementation (After P1)
**Created:** 2025-10-25
**Last Updated:** 2025-10-25
**Prerequisites:** P1 tasks complete (78-80% coverage)
